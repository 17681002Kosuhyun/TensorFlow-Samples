{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "      <th>preddef1</th>\n",
       "      <th>preddef2</th>\n",
       "      <th>preddef3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>176</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "      <td>0.808394</td>\n",
       "      <td>0.788640</td>\n",
       "      <td>0.213043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198297</td>\n",
       "      <td>0.128445</td>\n",
       "      <td>0.436903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>55</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.856075</td>\n",
       "      <td>2.168925</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.141023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>120</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.104422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "      <td>0.781588</td>\n",
       "      <td>0.737885</td>\n",
       "      <td>0.436903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  ed  employ  address  income  debtinc   creddebt   othdebt default  \\\n",
       "0   41   3      17       12     176      9.3  11.359392  5.008608       1   \n",
       "1   27   1      10        6      31     17.3   1.362202  4.000798       0   \n",
       "2   40   1      15       14      55      5.5   0.856075  2.168925       0   \n",
       "3   41   1      15       14     120      2.9   2.658720  0.821280       0   \n",
       "4   24   2       2        0      28     17.3   1.787436  3.056564       1   \n",
       "\n",
       "   preddef1  preddef2  preddef3  \n",
       "0  0.808394  0.788640  0.213043  \n",
       "1  0.198297  0.128445  0.436903  \n",
       "2  0.010036  0.002987  0.141023  \n",
       "3  0.022138  0.010273  0.104422  \n",
       "4  0.781588  0.737885  0.436903  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_data_1 = pd.read_csv('bankloanData.csv')\n",
    "df_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "ed            int64\n",
       "employ        int64\n",
       "address       int64\n",
       "income        int64\n",
       "debtinc     float64\n",
       "creddebt    float64\n",
       "othdebt     float64\n",
       "default      object\n",
       "preddef1    float64\n",
       "preddef2    float64\n",
       "preddef3    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41, 3, 17, ..., 0.808394327359702, 0.7886404318214371,\n",
       "        0.21304337612811897],\n",
       "       [27, 1, 10, ..., 0.19829747615910395, 0.128445387038174,\n",
       "        0.43690300550604605],\n",
       "       [40, 1, 15, ..., 0.0100361080990023, 0.00298677834821412,\n",
       "        0.141022623460993],\n",
       "       ..., \n",
       "       [48, 1, 13, ..., 0.0301374981044824, 0.0325702625943738,\n",
       "        0.24801041775523303],\n",
       "       [35, 2, 1, ..., 0.26900345101699397, 0.37854649636973203,\n",
       "        0.181814378077261],\n",
       "       [37, 1, 20, ..., 0.006397812918809229, 0.0111731232851226,\n",
       "        0.30304155578236497]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make a numpy array from the dataframe, except remove rows with no value for 'default'\n",
    "i = list(df_data_1.columns.values).index('default')\n",
    "data = np.array([x for x in df_data_1.values if x[i] in ['0', '1']])\n",
    "\n",
    "# Remove the columns for preddef1, predef2 and preddef3\n",
    "data = np.delete(data, slice(9,12), axis=1)\n",
    "\n",
    "# Separate the 'predictors' (aka 'features') from the dependent variable (aka 'label') that we will learn how to predict\n",
    "predictors = np.delete(data, 8, axis=1)\n",
    "dependent = np.delete(data, slice(0, 8), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the label type to numeric categorical representing the classes to predict (binary classfier)\n",
    "dependent = dependent.astype(int)\n",
    "\n",
    "# And flatten it to one dimensional for use as the expected output label vector in TensorFlow\n",
    "dependent = dependent.flatten()\n",
    "dependent\n",
    "\n",
    "# Convert all the predictors to float to simplify this demo TensorFlow code\n",
    "predictors = predictors.astype(float)\n",
    "\n",
    "# Get the shape of the predictors\n",
    "m, n = predictors.shape\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  41.      ,    3.      ,   17.      ,   12.      ,  176.      ,\n",
       "          9.3     ,   11.359392,    5.008608])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partition the input data into a training set and a test set\n",
    "\n",
    "m_train = 500\n",
    "m_test = m - m_train\n",
    "\n",
    "predictors_train = predictors[:m_train]\n",
    "dependent_train = dependent[:m_train]\n",
    "\n",
    "predictors_test = predictors[m_train:]\n",
    "dependent_test = dependent[m_train:]\n",
    "\n",
    "# Gets a batch of the training data. \n",
    "# NOTE: Rather than loading a whole large data set as above and then taking array slices as done here, \n",
    "#       This method can connect to a data source and select just the batch needed.\n",
    "def get_training_batch(batch_num, batch_size):\n",
    "    lower = batch_num * (m_train // batch_size)\n",
    "    upper = lower + batch_size\n",
    "    return predictors_train[lower:upper], dependent_train[lower:upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Make this notebook's output stable across runs\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# A method to build a new neural net layer of a given size,  \n",
    "# fully connect it to a given preceding layer X, and \n",
    "# compute its output Z either with or without (default) an activation function\n",
    "# Call with activation=tf.nn.relu or tf.nn.sigmoid or tf.nn.tanh, for examples\n",
    "\n",
    "def make_nn_layer(layer_name, layer_size, X, activation=None):\n",
    "    with tf.name_scope(layer_name):\n",
    "        X_size = int(X.get_shape()[1])\n",
    "        SD = 2 / np.sqrt(X_size)\n",
    "        weights = tf.truncated_normal((X_size, layer_size), dtype=tf.float64, stddev=SD)\n",
    "        W = tf.Variable(weights, name='weights')\n",
    "        b = tf.Variable(tf.zeros([layer_size], dtype=tf.float64), name='biases')\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the neural net structure\n",
    "\n",
    "n_inputs = n\n",
    "n_hidden1 = n \n",
    "### n_hidden2 = n // 2\n",
    "n_outputs = 2   # Two output classes: defaulting or non-defaulting on loan\n",
    "\n",
    "X = tf.placeholder(tf.float64, shape=(None, n_inputs), name='X')\n",
    "\n",
    "with tf.name_scope('nn'):\n",
    "    hidden1 = make_nn_layer('hidden1', n_hidden1, X, activation=tf.nn.relu)\n",
    "    hidden2 = hidden1\n",
    "    ### hidden2 = make_nn_layer('hidden2', n_hidden2, hidden1, activation=tf.nn.relu)\n",
    "    outputs = make_nn_layer('outputs', n_outputs, hidden2) \n",
    "    outputs = tf.identity(outputs, \"nn_output\")\n",
    "    \n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define how the neural net will learn\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n",
    "    loss = tf.reduce_mean(xentropy, name='l')\n",
    "    \n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"test\"):\n",
    "    correct = tf.nn.in_top_k(tf.cast(outputs, tf.float32), y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the ability to save and restore the trained neural net...\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... and make a subdirectory space in which to save the model files (only need to run this once)\n",
    "!mkdir \"../datasets\"\n",
    "!mkdir \"../datasets/Neural Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'Training accuracy:', 0.75999999, 'Testing accuracy:', 0.755)\n",
      "(200, 'Training accuracy:', 0.77399999, 'Testing accuracy:', 0.79000002)\n",
      "(300, 'Training accuracy:', 0.79799998, 'Testing accuracy:', 0.81999999)\n",
      "(400, 'Training accuracy:', 0.79799998, 'Testing accuracy:', 0.83499998)\n",
      "(500, 'Training accuracy:', 0.80800003, 'Testing accuracy:', 0.82499999)\n",
      "(600, 'Training accuracy:', 0.796, 'Testing accuracy:', 0.815)\n",
      "(700, 'Training accuracy:', 0.80199999, 'Testing accuracy:', 0.81)\n",
      "(800, 'Training accuracy:', 0.78799999, 'Testing accuracy:', 0.815)\n",
      "(900, 'Training accuracy:', 0.81, 'Testing accuracy:', 0.815)\n",
      "(1000, 'Training accuracy:', 0.81999999, 'Testing accuracy:', 0.81)\n",
      "(1100, 'Training accuracy:', 0.81800002, 'Testing accuracy:', 0.81)\n",
      "(1200, 'Training accuracy:', 0.80599999, 'Testing accuracy:', 0.80500001)\n",
      "(1300, 'Training accuracy:', 0.80199999, 'Testing accuracy:', 0.815)\n",
      "(1400, 'Training accuracy:', 0.81199998, 'Testing accuracy:', 0.81999999)\n",
      "(1500, 'Training accuracy:', 0.80599999, 'Testing accuracy:', 0.80000001)\n",
      "(1600, 'Training accuracy:', 0.81599998, 'Testing accuracy:', 0.81)\n",
      "(1700, 'Training accuracy:', 0.80599999, 'Testing accuracy:', 0.81)\n",
      "(1800, 'Training accuracy:', 0.82200003, 'Testing accuracy:', 0.82499999)\n",
      "(1900, 'Training accuracy:', 0.80199999, 'Testing accuracy:', 0.80500001)\n",
      "(2000, 'Training accuracy:', 0.82999998, 'Testing accuracy:', 0.81999999)\n",
      "(2100, 'Training accuracy:', 0.81199998, 'Testing accuracy:', 0.81)\n",
      "(2200, 'Training accuracy:', 0.824, 'Testing accuracy:', 0.82499999)\n",
      "(2300, 'Training accuracy:', 0.80400002, 'Testing accuracy:', 0.815)\n",
      "(2400, 'Training accuracy:', 0.81800002, 'Testing accuracy:', 0.815)\n",
      "(2500, 'Training accuracy:', 0.82800001, 'Testing accuracy:', 0.82499999)\n",
      "(2600, 'Training accuracy:', 0.82999998, 'Testing accuracy:', 0.815)\n",
      "(2700, 'Training accuracy:', 0.82200003, 'Testing accuracy:', 0.815)\n",
      "(2800, 'Training accuracy:', 0.81999999, 'Testing accuracy:', 0.80500001)\n",
      "(2900, 'Training accuracy:', 0.81400001, 'Testing accuracy:', 0.82999998)\n",
      "(3000, 'Training accuracy:', 0.82599998, 'Testing accuracy:', 0.82499999)\n",
      "\n",
      "('Actual classes:   ', array([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]))\n",
      "('Predicted classes:', array([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# TRAINING TIME\n",
    "\n",
    "# This is how many times to use the full set of training data\n",
    "n_epochs = 3000\n",
    "\n",
    "# For a larger training set, it's typically necessary to break training into\n",
    "# batches so only the memory needed to store one batch of training data is used\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as training_session:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Shuffling (across batches) is easier to do for small data sets and\n",
    "        # helps increase accuracy\n",
    "        training_set = [[pt_elem, dependent_train[i]] for i, pt_elem in enumerate(predictors_train)]\n",
    "        np.random.shuffle(training_set)\n",
    "        predictors_train = [ts_elem[0] for ts_elem in training_set]\n",
    "        dependent_train = [ts_elem[1] for ts_elem in training_set]\n",
    "        \n",
    "        # Loop through the whole training set in batches\n",
    "        for batch_num in range(m_train // batch_size):\n",
    "            X_batch, y_batch = get_training_batch(batch_num, batch_size)\n",
    "            training_session.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        if epoch % 100 == 99:\n",
    "            acc_train = accuracy.eval(feed_dict={X: predictors_train, y: dependent_train})\n",
    "            acc_test = accuracy.eval(feed_dict={X: predictors_test, y: dependent_test})\n",
    "            print(epoch+1, \"Training accuracy:\", acc_train, \"Testing accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(training_session, \"../datasets/Neural Net/Neural Net.ckpt\")\n",
    "    \n",
    "    # A quick test with the trained model \n",
    "    Z = outputs.eval(feed_dict={X: predictors_test[:20]})\n",
    "    dependent_pred = np.argmax(Z, axis=1)\n",
    "    print(\"\")\n",
    "    print(\"Actual classes:   \", dependent_test[:20])  \n",
    "    print(\"Predicted classes:\", dependent_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../datasets/Neural Net/Neural Net.ckpt\n",
      "('Actual classes:   ', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1]))\n",
      "('Predicted classes:', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1]))\n",
      "\n",
      "('Confidences: ', [0.72115788841227302, 0.80607047578305713, 0.83934041973914708, 0.99522251895376912, 0.63583707224033514, 0.92301370061024191, 0.9246727235093638, 0.78439759502501416, 0.99999999999583422, 0.64561868763996888, 0.72736408931601459, 0.64561868763996888, 0.83959328566375235, 0.94291801514712859, 0.68170960394665236, 0.99981345698850999, 0.96778595641239296, 0.59248611260528017, 0.99293831799558985, 0.52204835458766652])\n",
      "\n",
      "('Probabilities: ', array([[  7.21157888e-01,   2.78842112e-01],\n",
      "       [  8.06070476e-01,   1.93929524e-01],\n",
      "       [  8.39340420e-01,   1.60659580e-01],\n",
      "       [  9.95222519e-01,   4.77748105e-03],\n",
      "       [  6.35837072e-01,   3.64162928e-01],\n",
      "       [  9.23013701e-01,   7.69862994e-02],\n",
      "       [  9.24672724e-01,   7.53272765e-02],\n",
      "       [  7.84397595e-01,   2.15602405e-01],\n",
      "       [  1.00000000e+00,   4.16586334e-12],\n",
      "       [  3.54381312e-01,   6.45618688e-01],\n",
      "       [  7.27364089e-01,   2.72635911e-01],\n",
      "       [  3.54381312e-01,   6.45618688e-01],\n",
      "       [  8.39593286e-01,   1.60406714e-01],\n",
      "       [  9.42918015e-01,   5.70819849e-02],\n",
      "       [  6.81709604e-01,   3.18290396e-01],\n",
      "       [  9.99813457e-01,   1.86543011e-04],\n",
      "       [  9.67785956e-01,   3.22140436e-02],\n",
      "       [  4.07513887e-01,   5.92486113e-01],\n",
      "       [  9.92938318e-01,   7.06168200e-03],\n",
      "       [  4.77951645e-01,   5.22048355e-01]]))\n"
     ]
    }
   ],
   "source": [
    "# Restore the saved model and use it to perform inference on a \"received\" new set of data\n",
    "\n",
    "# We will simulate \"receiving\" the new data by taking a slice of the test set.\n",
    "predictors_received = predictors_test[20:40]\n",
    "\n",
    "import tensorflow as tf_inference\n",
    "\n",
    "with tf_inference.Session() as inference_session:\n",
    "    inf_saver = tf_inference.train.import_meta_graph('../datasets/Neural Net/Neural Net.ckpt.meta')\n",
    "    inf_saver.restore(inference_session, tf_inference.train.latest_checkpoint('../datasets/Neural Net/'))\n",
    "    \n",
    "    graph = tf_inference.get_default_graph()    \n",
    "    nn_output = graph.get_tensor_by_name(\"nn/nn_output:0\")\n",
    "\n",
    "    Z = inference_session.run(nn_output, feed_dict={X: predictors_received})\n",
    "    dependent_pred = np.argmax(Z, axis=1)\n",
    "    \n",
    "    dependent_prob = inference_session.run(tf_inference.nn.softmax(nn_output), feed_dict={X: predictors_received})\n",
    "\n",
    "    confidences = [p[dependent_pred[i]] for i, p in enumerate(dependent_prob)]\n",
    "    \n",
    "print(\"Actual classes:   \", dependent_test[20:40])\n",
    "print(\"Predicted classes:\", dependent_pred)\n",
    "print(\"\")\n",
    "print(\"Confidences: \", confidences)\n",
    "print(\"\")\n",
    "print(\"Probabilities: \", dependent_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For when you want to wipe out the training and do it again\n",
    "!rm -rf \"../datasets/Neural Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 80\r\n",
      "-rw-r--r--  1 boyerj  staff    720 10 Nov 13:26 Neural Net.ckpt.data-00000-of-00001\r\n",
      "-rw-r--r--  1 boyerj  staff    238 10 Nov 13:26 Neural Net.ckpt.index\r\n",
      "-rw-r--r--  1 boyerj  staff  25750 10 Nov 13:26 Neural Net.ckpt.meta\r\n",
      "-rw-r--r--  1 boyerj  staff     87 10 Nov 13:26 checkpoint\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l \"../datasets/Neural Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -l \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf \"../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
